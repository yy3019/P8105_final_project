---
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 10, 
                      fig.align = "center"
                      )

# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "dplyr", "rvest", "httr")
invisible(lapply(Packages, library, character.only = TRUE))



# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + 
            theme(legend.position = "bottom") +
            theme(plot.title = element_text(hjust = 0.5, size = 12),
                  plot.subtitle = element_text(hjust = 0.5, size = 8))
          )

```

# Motivation

Our motivation behind this project is to explore the fragility index (FI) beyond what those involved in clinical trials have done. FI has been suggested as an easy-to-understand metric that bridges the intent of researchers and utilization by clinicians. It has growing interest in the oncology community but we want to explore its utility beyond this scope. You can go [here](proj_background.html) to learn more about our motivation. 

**Note:** You can expand our code chunks by clicking on `code` on the right pill-button.
<br></br>

### Some facts


<br></br>

### Primary Goals
* Explore general trend of FI in recent phase III clinical trials   
* Construct a web-based FI calculator which allows user to input their own dataset       
* Discover potential impact factors on FI. e.g. disease type, treatment type, conducted location

<br></br>

# Related work

Our study is inspired mainly by three researches. First of all, the [Walsh study](https://www.jclinepi.com/article/S0895-4356(13)00466-6/abstract) has general introduction on Fragility Index and methodology behind. The author also provided an example of utilizing FI to evaluate the roubustness of a set of 399 eligible clinical trials.      
[Another study](https://jamanetwork.com/journals/jamasurgery/fullarticle/2712857?fbclid=IwAR0bb9aN2gwCOFd-HLF-DFOmT60wBtziuFZ_CKdxIu-y40eXYb9GKa8LhcU) done by Tignanelli also acknowleged FI as crucial part to be reported in further trama/ surgical RCT studies.       
Lastly, the [Del Paggio study](https://www.thelancet.com/journals/lanonc/article/PIIS1470-2045(19)30338-9/fulltext) revealed a trembling fact that many of the FDA-approved phase III clinical studies had a low FI; some of them were even 1% or less of their respective sample size. These materials serve as the inspiration of our topic.

<br></br>

# Initial Questions

As our initial goal, we want to find the relationship between fragility index and disease type in phase III RCT. To be more specific, we want to see if there's significant difference in FI between trials targeting cancers and those targeting non-oncology immunologic disorders. Also, we would like to discover any other facotrs that may impact the FI of RCT studies. However, during the data collection process, we find out that all the criterias we've applied would result in insufficient sample size to conduct further analysis. Accordingly, we've enlarged our filtering criteria to US based phase III clinical trials.     
On the other hand, given the dataset we scraped, we're allowed to perform exploratory data analysis on the study sites, published journal and their respective FI.

<br></br>

# Obtaining Data

### Attempt to Scrape from Google Scholar

Given the ease of access to Google Scholar (GS), we first attempted to scrape our sources from the site. We initially did this manually by searching some random entry in order to see a pattern where we can insert our "page number". We found that we need to split Google's search address into two partitions so that we can insert the search numbers. 

```{r gs_scrape}

# have to split link into 2 parts...
gs_url_base1 <- "https://scholar.google.com/scholar?start="
gs_url_base2 <- "&q=monoclonal+antibody+phase+3&hl=en&as_sdt=0,33&as_ylo=2007&as_yhi=2017&lookup=0"

# adds the search term by 10s (obtained by evaluating scholar's http address)
gs_vec_url <- str_c(gs_url_base1, seq(0, 100, 10), gs_url_base2)

```

Once we figured out the web address components, we attempted to incorporate this into our scraping function by using `purrr::map` function and search for the term "monoclonal antibody phase 3". Utilizing ["SelectorGadget"](https://selectorgadget.com/), we were able to obtain the css tag for the title, `.gs_rt a` and used this to scrape our articles/journals. 

```{r gs_scraping_fn, eval = FALSE}

# GS scraper fn
read_page <- function(url) {
  
  h = read_html(url)               # reads url input
  
  title = h %>%
    html_nodes(".gs_rt a") %>%     # pulls the specific html tag (for titles)
    html_text()
  
  data_frame(title)                # turns scraped data into a dataframe
}

# map read test
gs_test <- map(gs_vec_url, read_page)

# unnested df test (success)
unnested_gs <- gs_test %>% 
  tibble::enframe(name = NULL) %>% 
  unnest()

# peek into the first 3 results
head(unnested_gs, n = 3L)

```

The code we made was successful in scraping the titles from GS. However, an apparent issue with this is that we're unable to get the links to the actual paper. Furthermore, the various sources of the article will be highly varied when accessing these articles, which complicates the generalizability of our functions. Additionally, we're unable to exclusively scrape the year of publication, which further worsens this method. 

Given the apparent high ceiling to scrape via GS, we decided to stop using GS and tried PubMed scraping.

### Scraping from PubMed

Attempting to scrape PubMed results for our relevant issues quickly comes to a halt because we noticed that as we try to search results, their html address does **NOT** change or provide a page number. We quickly scraped this from our viable methods to obtain our data. 

### Scraping from clinicaltrials.gov

#### Scraping Clinical Trial ID

Our last bastion, [clinicaltrials.gov](https://clinicaltrials.gov), were fortunately successful (in some ways). We ended up employing a combination of API search to obtain specific trial ID and use those to scrape relevant contents from clinicaltrials.gov. You can see our codes and process in detail "link"

We first obtained (downloaded) a `.csv` file from clinicaltrials.gov that contained our advanced search options:

* phase III trials 
* completed

After obtaining this file, we read it into R and clean-up the relevant part of this file. In particular, we have to clean the provided url as we're only interested in the trial ID. We further filter the trials to those that has a "Placebo" arm. 

```{r ctgov_data_read}

ctgov_scrape_test <- read_csv("./data/ctgov_test_API.csv") %>%      # read csv
  janitor::clean_names() %>% 
  mutate(
    url = str_replace(url, "https://ClinicalTrials.gov/show/", "")    # keep trial ID only
  ) %>% 
  rename("nct_id" = url) %>% 
  select(rank, nct_id, title, conditions, interventions) %>%          # remove location; irrelevant
  filter(str_detect(interventions, "Placebo"))


## disabled code to check distinct categories of potential interest:
#  ctgov_scrape_test %>% 
#  distinct(conditions) %>% 
#  view()

```

Now our "database" containing relevant clinical trials are ready. Before we continue, we need to obtain the html tags from clinicaltrials.gov which contains our variables of interest. With the help of SelectorGadget, we managed to isolate the tags to contain:

* `#EXPAND-outcome-data-1 .labelSubtle` : label within the table this is the thing that needs to have participants or "Unit of Measure: Participants"
* `#EXPAND-outcome-data-1 td.de-outcomeLabelCell` : this is arm description for primary outcome
* `#EXPAND-outcome-data-1 tbody:nth-child(2) th` : this is description of table

These tags will be used to identify certain cells in the messy table within clinicaltrials.gov. 
<br></br>

#### Testing Data Scraping and Transformation to Dataframe

We then test our code to be able to run and create a dataframe from a single source. It took quite a few trial and errors to be able to obtain the right form. 

Firstly, we attempt to find an identifier relevant to our project that may help us automate the determination of which trial will be included. 

```{r ctgov_test_id, eval = FALSE}

# test to obtain the particular unit of measure; "participants"
test_url_ctgov <- read_html("https://ClinicalTrials.gov/show/results/NCT00195702") %>%
  html_nodes("#EXPAND-outcome-data-1 .labelSubtle") %>% 
  html_text() %>% 
  str_replace("Unit of Measure: ", "")

```

This returns our important "unit of measure". We are particularly looking for variables that contain "participants". Having found a way to detect this, we continued to build our function under the assumption that we have "participants" as our unit of measure. 

We obtained the relevant column names:

```{r col_names_pull}

# pulling the relevant column names that corresponds to the eventual numbers we pulled.
# this always has the first term be "\n    Arm/Group Title \n  ", which we will ignore
col_names_test <- read_html("https://ClinicalTrials.gov/show/results/NCT00195702") %>% 
  html_nodes("#EXPAND-outcome-data-1 tbody:nth-child(2) tr:nth-child(1) .de-outcomeLabelCell") %>%
  html_text()

col_names_test

```

The row names, in case we need it as well:

```{r row_names_pull}

# obtains relevant row names
# ideally this will be participants and the specified units of measure
row_names_test <- read_html("https://ClinicalTrials.gov/show/results/NCT00195702") %>% 
  html_nodes("#EXPAND-outcome-data-1 .labelSubtle , #EXPAND-outcome-data-1 tbody:nth-child(2) tr:nth-child(4) .de-outcomeLabelCell") %>%
  html_text()

row_names_test

```

And finally, our most important component, which is the number of participants in each group. 

```{r content_pull}

# scrape the actual data, with variable table length
content_test <- read_html("https://ClinicalTrials.gov/show/results/NCT00195702") %>% 
  html_nodes("#EXPAND-outcome-data-1 .de-numValue_outcomeDataCell") %>%
  html_text() %>% 
  as.numeric() %>%                           # cleans our data to contain just the numbers
  matrix(ncol = length(col_names_test[-1]),  # make into a matrix of correct size 
         nrow = length(row_names_test), 
         byrow = TRUE) %>% 
  as_tibble()                                # convert the matrix into a tibble

# add column names to our df
names(content_test) <- col_names_test[-1]

# adds rownames to our tibbles
# a tibble doesnt show rownames when you call it but it is saved into R
rownames(content_test) <- row_names_test

content_test

```

Though it took a while, our code managed to pull the relevant info that we need to manipulate in order to evaluate our fragility indexes. Our next step then, is to be able to `map` this and make a listcol within our original dataframe. 

#### Building the function with the proper conditions and mapping 

Similar to how we approach making the single-url code, we had various trial and error to come up with the function. We start with a simple form of object that stores our url. We extract the "unit of measure" and evaluate whether or not it contains "participants" or "patients"; scrape if `TRUE`, return `NA` if `FALSE`. 

A slightly different step was to split the content-scraping and data-frame transformation into two separate steps in order to clarify the segmentation. 

```{r function_build_working}

# function to scrape from url
outcome_extractor <- function(data) {
  
  # stores our list of urls
  site = read_html(str_c("https://ClinicalTrials.gov/show/results/", data))
  
  # pulls our "measure" detector
  measure = site %>%
    html_nodes("#EXPAND-outcome-data-1 .labelSubtle") %>% 
    html_text() %>% 
    str_replace("Unit of Measure: ", "")
  
  # if/else function that skips to the next url if our measure does not contain "participants" or "patients"
  if (length(measure) == 0) {      # added after bug found; accounts for length = 0
    NA
  } else if (!str_detect(measure, "[pP]articipants|[pP]atients")) {
    NA            # returns NA if it doesn't exist
  } else {
    
    # pulls relevant column names
    col_names = site %>%
      html_nodes("#EXPAND-outcome-data-1 tbody:nth-child(2) tr:nth-child(1) .de-outcomeLabelCell") %>%
      html_text()
    
    # pulls row names (might be unnecessary)
    row_names = site %>% 
      html_nodes("#EXPAND-outcome-data-1 .labelSubtle , #EXPAND-outcome-data-1 tbody:nth-child(2) tr:nth-child(4) .de-outcomeLabelCell") %>% 
      html_text()
    
    # pulls the actual numbers (content)
    content = site %>% 
      html_nodes("#EXPAND-outcome-data-1 .de-numValue_outcomeDataCell") %>% 
      html_text() %>% 
      as.numeric()
    
    # makes a dataframe using matrix
    df = matrix(content, 
                ncol = length(col_names[-1]), 
                nrow = length(row_names), 
                byrow = TRUE) %>% 
      as_tibble() 
    
    # adds column names
    names(df) <- col_names[-1]
    #rownames(df) <- row_names
    
    # returns the df output
    df
  }
}

```

We then test this by scraping our .csv file, `ctgov_test_API.csv` located inside our `data` folder (loaded earlier) and map our newly-made function. A critical step after we tested this was that we need to filter our `NA` for our listcol to appear proper. 

```{r map_fn_test}

# testing our function using purrr::map function.
mapped_ctgov_scrape_test <- ctgov_scrape_test %>% 
  mutate(
    scrape_data = map(nct_id, outcome_extractor)
  ) %>% 
  filter(!is.na(scrape_data))               # necessary step to clear out the NA rows.

```

#### Mapping our real dataset

Now that we have a working function that produces our expected extraction, we can use our real data source, `ctgov_10k_API`. This time, we just apply our previous functions and wait for about 30-40 minutes to "compile". 

**Note:** During one of our tries, we realize that there's an extra element in the "outcome measures" that we failed to take into account; `character(0)`. We had to debug this for a while to make our function work by adding another condition: `if(length(input) == 0)`, which returns `NA` if `TRUE`.

**TURN THE CODE OFF UNLESS READY TO BUILD WITH SIGNIFICANT UPDATES**

```{r ctgov_real_scrape}

ctgov_10k_df <- read_csv("./data/ctgov_10k_API.csv") %>%              # read csv
  janitor::clean_names() %>%
  mutate(
    url = str_replace(url, "https://ClinicalTrials.gov/show/", "")    # keep trial ID only
  ) %>% 
  rename("nct_id" = url) %>% 
  select(rank, nct_id, title, conditions, interventions) %>%     
  filter(str_detect(interventions, "Placebo"))


# testing our function using purrr::map function.
ctgov_10k_scraped <- ctgov_10k_df %>% 
  mutate(
    scrape_data = map(nct_id, outcome_extractor)
  ) %>% 
  filter(!is.na(scrape_data))               # necessary step to clear out the NA rows.

```

After all this, our resulting viable dataset is `r nrow(ctgov_10k_scraped)` datasets that each contains the primary outcomes of interest. There are `r nrow(ctgov_10k_scraped %>% distinct(conditions))` unique conditions found using `distinct()` function. 

### Cleaning Data

Cleaning the data surprisingly wasn't as bad as we thought. However, we do have to....

Our process is explained in detail "link"

# Exploratory Analysis
Exploratory analysis: Visualizations, summaries, and exploratory statistical analyses. Justify the steps you took, and show any major changes to your ideas.

### Preliminary exploration from our scraped dataset

```{r count_of_unique}

cond_count <- ctgov_10k_scraped %>% 
  count(conditions)

```


# Other Analysis

Additional analysis: If you undertake formal statistical analyses, describe these in detail

# Discussion

Discussion: What were your findings? Are they what you expect? What insights into the data can you make?
