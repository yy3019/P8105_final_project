---
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 10, 
                      fig.align = "center"
                      )

# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "dplyr", "rvest", "httr")
invisible(lapply(Packages, library, character.only = TRUE))



# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + 
            theme(legend.position = "bottom") +
            theme(plot.title = element_text(hjust = 0.5, size = 12),
                  plot.subtitle = element_text(hjust = 0.5, size = 8))
          )

```

# Motivation

Our motivation behind this project is to explore the fragility index (FI) beyond what those involved in clinical trials have done. FI has been suggested as an easy-to-understand metric that bridges the intent of researchers and utilization by clinicians. It has growing interest in the oncology community but we want to explore its utility beyond this scope. You can go [here](proj_background.html) to learn more about our motivation. 
<br></br>

### Some facts
<br></br>

### Primary Goals
<br></br>

# Related work

insert prior research?
<br></br>

# Initial Questions

Initial questions: What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis?
<br></br>

# Obtaining Data

### Attempt to Scrape from Google Scholar

```{r google_schol_scrape}

# have to split link into 2 parts...
gs_url_base1 <- "https://scholar.google.com/scholar?start="
gs_url_base2 <- "&q=monoclonal+antibody+phase+3&hl=en&as_sdt=0,33&as_ylo=2007&as_yhi=2017&lookup=0"

# adds the search term by 10s (obtained by evaluating scholar's http address)
gs_vec_url <- str_c(gs_url_base1, seq(0, 100, 10), gs_url_base2)

```

Given the ease of access to Google Scholar (GS), we first attempted to scrape our sources from the site. We initially did this manually by searching some random entry in order to see a pattern where we can insert our "page number". We found that we need to split Google's search address into two partitions so that we can insert the search numbers. 

Once we figured out the web address components, we attempted to incorporate this into our scraping function by using `purrr::map` function and search for the term "monoclonal antibody phase 3". Utilizing ["SelectorGadget"](https://selectorgadget.com/), we were able to obtain the css tag for the title, `.gs_rt a` and used this to scrape our articles/journals. 

```{r scraping_fn}

# scraper fn
read_page <- function(url) {
  
  h = read_html(url)               # reads url input
  
  title = h %>%
    html_nodes(".gs_rt a") %>%     # pulls the specific html tag (for titles)
    html_text()
  
  data_frame(title)                # turns scraped data into a dataframe
}

# map read test
gs_test <- map(gs_vec_url, read_page)

# unnested df test (success)
unnested_gs <- gs_test %>% 
  tibble::enframe(name = NULL) %>% 
  unnest()

# peek into the first 3 results
head(unnested_gs, n = 3L)

```

As seen above, the code was successful in scraping the titles from GS. However, an apparent issue with this is that we're unable to get the links to the actual paper. Furthermore, the various sources of the article will be highly varied when accessing these articles, which complicates the generalizability of our functions. Additionally, we're unable to exclusively scrape the year of publication, which further worsens this method. 

Given the apparent high ceiling to scrape via GS, we decided to stop using GS and tried PubMed scraping.

### Scraping from PubMed
Attempting to scrape PubMed results for our relevant issues quickly comes to a halt because we noticed that as we try to search results, their html address does **NOT** change or provide a page number. We quickly scraped this from our viable methods to obtain our data. 

### Scraping from clinicaltrials.gov

##### Scraping Clinical Trial ID

Our last bastion, [clinicaltrials.gov](https://clinicaltrials.gov), were fortunately successful (in some ways). We ended up employing a combination of API search to obtain specific trial ID and use those to then scrape relevant contents from clinicaltrials.gov. You can see our codes and process in detail "link"

We first obtained (downloaded) a `.csv` file from clinicaltrials.gov that contained our advanced search options:

* options...

After obtaining this file, we read it into R and clean-up the relevant part of this file. In particular, we have to clean the provided url as we're only interested in the trial ID. We further filter the trials to those that has a "Placebo" arm. 

```{r ctgov_data_read}

ctgov_scrape_test <- read_csv("./data/SearchResults(1).csv") %>%      # read csv
  janitor::clean_names() %>% 
  mutate(
    url = str_replace(url, "https://ClinicalTrials.gov/show/", "")    # keep trial ID only
  ) %>% 
  rename("nct_id" = url) %>% 
  select(rank, nct_id, title, conditions, interventions) %>%          # remove location; irrelevant
  filter(str_detect(interventions, "Placebo"))

ctgov_scrape_test %>% 
  distinct(conditions) %>% 
  view()

```

Now our "database" that contains relevant clinical trials are ready. Before we continue, we need to obtain the html tags from clinicaltrials.gov which contains our variables of interest. With the help of SelectorGadget, we managed to isolate the tags to contain:

* `#EXPAND-outcome-data-1 .labelSubtle` : label within the table this is the thing that needs to have participants or "Unit of Measure: Participants"
* `#EXPAND-outcome-data-1 td.de-outcomeLabelCell` : this is arm description for primary outcome
* `#EXPAND-outcome-data-1 tbody:nth-child(2) th` : this is description of table

These tags will be used to identify certain cells in the messy table within clinicaltrials.gov. 

##### Testing Data Scraping and Transformation to Dataframe

First, we 
```{r ctgov_test1}

# test to obtain the particular unit of measure; "participants"
test_url_1 <- read_html("https://ClinicalTrials.gov/show/results/NCT00195702") %>%
  html_nodes("#EXPAND-outcome-data-1 .labelSubtle") %>% 
  html_text() %>% 
  str_replace("Unit of Measure: ", "")

# pulling the relevant column names that corresponds to the eventual numbers we pulled.
# this always has the first term be "\n    Arm/Group Title \n  ", which we will ignore
col_names <- read_html("https://ClinicalTrials.gov/show/results/NCT00195702") %>% 
  html_nodes("#EXPAND-outcome-data-1 tbody:nth-child(2) tr:nth-child(1) .de-outcomeLabelCell") %>%
  html_text()

col_names

#important for rownames #ideally this should be participants and unit of measure
row_names <- read_html("https://ClinicalTrials.gov/show/results/NCT00195702") %>% 
  html_nodes("#EXPAND-outcome-data-1 .labelSubtle , #EXPAND-outcome-data-1 tbody:nth-child(2) tr:nth-child(4) .de-outcomeLabelCell") %>%
  html_text()

row_names

#scrape the actual data, with variable table length
test123 <- read_html("https://ClinicalTrials.gov/show/results/NCT00195702") %>% 
  html_nodes("#EXPAND-outcome-data-1 .de-numValue_outcomeDataCell") %>%
  html_text()

#get rid of dumb characters
test123 <- test123 %>% as.numeric()

#make into a matrix of correct size 

my_matrix<- matrix(test123, ncol=length(col_names[-1]), nrow=length(row_names), byrow=TRUE)
my_matrix

my_df<- as.tibble(my_matrix)
names(my_df) <- col_names[-1]
#a tibble doesnt show rownames when you call it but it is saved into R
rownames(my_df) <- row_names


#what we should do next is store this into a listcol, and then iterate to the next website!
```

### Cleaning Data

Cleaning the data surprisingly wasn't as bad as we thought. However, we do have to....

Our process is explained in detail "link"

# Exploratory Analysis
Exploratory analysis: Visualizations, summaries, and exploratory statistical analyses. Justify the steps you took, and show any major changes to your ideas.

# Other Analysis

Additional analysis: If you undertake formal statistical analyses, describe these in detail

# Discussion

Discussion: What were your findings? Are they what you expect? What insights into the data can you make?
